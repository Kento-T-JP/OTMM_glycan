Google Colaboratoryの実行結果はCPUなどの環境がすべて同じとは限らないので、学習済みパラメータと尤度の推移のみ利用する。

Google Colaboratoryの実行時間は参考程度にすること！

また、データ数のmaxは5195である。

学習のエポック数（繰り返し数）は「likelihoodの数-1」である（最初の尤度は初期確率分布の尤度）。

＜用語＞
ColabFree・・・Google Colaboratoryの無料プランでの実行結果

＜注意＞
コンピュータでの計算ではビットを使っていることやlog(x+y)で近似を使っていることから、誤差によって確率がわずかに1を超える場合がある。
しかし、その様な場合は他の確率が限りなく0に近くなるので、心配をする必要はない。
Excelでsumを取っても全体として確率の合計は1になる（余りにも他の値が小さすぎて合計が1になる）。
本研究では、ざっくりと傾向がわかれば良いので、近似などを用いて厳密な計算をしていない。

＜浮動小数点＞
浮動小数点によって結果が大きく変わってしまう（かもしれない）。
対策としてdecimal.Decimalを使う必要があるが、Numpyはこのデータ型を認識しない。
よって、一旦値を取り出した後は、float()でfloat（浮動小数点）にした後、Decimal(str())でDecimal型にして計算を行う。
そして、正確に計算された結果を再度Numpyに戻す（Decimal型としては認識されない）
しかし、numpyに代入するときfloatになって値が丸め込まれてしまう。これによって、誤差が生まれてしまう。
よって、この方針は却下。
どうやらlist型とDataFrameはDecimalに対応しているので、Numpy型をlist型に変えて計算を行う。
これによって、代入を除く全ての演算でDecimalを扱うことができた。

＜結果＞
Decimalを使っても、自分のPCでは安定しない。
10^n倍して整数(int)にして計算しても、恐らくsmoothmaxの処理で浮動小数点の問題が発生して安定しない。
roundでまとめてもやはり、浮動小数点の問題で安定しない。
よって、Google colabやMac proなどの正常に動くパソコンを用いて実験をする。
※データ数が大きくなってもGoogleColabでは同じ結果である。

＜豆知識＞
delとかgc.collect()とかを消去するとめっちゃ速い。

＜考察＞
どんな手を尽くしても自分のPC（surface laptop2）はおかしな挙動をする（Google colabは予想通りの動きをする）。
具体的には、結果も毎回違う（再現性なし）、エポックを繰り返すと値が急に大きく（小さく）なる。
恐らくスペックが絶望的に足りないか、動的計画法を行うと簡単にメモリが溢れてバグってしまうのか、壊れているかの3択である。
ただ、このプログラムを実行する以外、不自由なく動くので壊れている可能性は考えにくい。
